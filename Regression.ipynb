{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1 What is Simple Linear Regression?\n",
        "-  Simple Linear Regression is a statistical method used to model the relationship between two continuous variables.\n"
      ],
      "metadata": {
        "id": "-ZIXaurwntpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 What are the key assumptions of Simple Linear Regression?\n",
        "-  Linearity: Relationship is a straight line.\n",
        "\n",
        "   Independence: Observations don't influence each other.\n",
        "\n",
        "   Homoscedasticity: Errors have constant variance.\n",
        "\n",
        "   Normality: Errors are normally distributed."
      ],
      "metadata": {
        "id": "NtPpArTxn2gG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3  What does the coefficient m represent in the equation Y=mX+c\n",
        "-  In the equation Y = mX + c, 'm' represents the slope of the line. It indicates how much the dependent variable (Y) changes for every one-unit increase in the independent variable (X).\n"
      ],
      "metadata": {
        "id": "Mb3Uwc8moNfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4  What does the intercept c represent in the equation Y=mX+c?\n",
        "-  In the equation Y = mX + c, 'c' represents the y-intercept. It is the value of the dependent variable (Y) when the independent variable (X) is zero. In other words, it's where the regression line crosses the y-axis."
      ],
      "metadata": {
        "id": "0FGp-khfoV51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5  How do we calculate the slope m in Simple Linear Regression?\n",
        "-  In Simple Linear Regression, the slope 'm' in the equation Y = mX + c is calculated using the following formula:\n",
        "\n",
        "$m = \\frac{\\sum((x_i - \\bar{x})(y_i - \\bar{y}))}{\\sum((x_i - \\bar{x})^2)}$$m = \\frac{\\sum((x_i - \\bar{x})(y_i - \\bar{y}))}{\\sum((x_i - \\bar{x})^2)}$"
      ],
      "metadata": {
        "id": "RO44-UcDof1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "-  The purpose of the least squares method in Simple Linear Regression is to find the line that best fits the data points. It does this by minimizing the sum of the squared vertical distances (residuals) between each data point and the line. In other words, it finds the line that has the smallest overall error between the predicted values (on the line) and the actual observed values."
      ],
      "metadata": {
        "id": "U_FCDk0ro06p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7 How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "-  The coefficient of determination, denoted as R², is a measure that represents the proportion of the variance in the dependent variable (Y) that is predictable from the independent variable (X). In simpler terms, it tells you how well the regression model fits the observed data."
      ],
      "metadata": {
        "id": "iiHjZcUVpGHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. What is Multiple Linear Regression?\n",
        "\n",
        "-  Multiple Linear Regression is an extension of simple linear regression. While simple linear regression models the relationship between a dependent variable and one independent variable, multiple linear regression models the relationship between a dependent variable and two or more independent variables."
      ],
      "metadata": {
        "id": "2VNy_DODpQpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "-  The main difference between Simple Linear Regression and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable."
      ],
      "metadata": {
        "id": "oIa-W73wpV5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10 What are the key assumptions of Multiple Linear Regression?\n",
        "-  Linearity: Linear relationship between dependent and independent variables.\n",
        "\n",
        "  Independence: Observations are independent.\n",
        "\n",
        "  Homoscedasticity: Errors have constant variance.\n",
        "\n",
        "  Normality: Errors are normally distributed.\n",
        "\n",
        "  No Multicollinearity: Independent variables are not highly correlated."
      ],
      "metadata": {
        "id": "ONukYu1npZvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11 What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "-  Heteroscedasticity: Variance of errors (residuals) is not constant across independent variables.\n",
        "\n",
        "  Effect: Does not bias coefficients but makes standard errors, p-values, and confidence intervals inaccurate. This leads to unreliable statistical significance tests."
      ],
      "metadata": {
        "id": "ppdZ7LJ9pymu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12 How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "-  To improve a model with high multicollinearity:\n",
        "\n",
        "  Remove one of the highly correlated variables.\n",
        "\n",
        "  Combine correlated variables into one.\n",
        "\n",
        "  Use dimensionality reduction (like PCA).\n",
        "\n",
        "  Consider Ridge or Lasso Regression.\n",
        "\n",
        "  collect more data (if possible).\n"
      ],
      "metadata": {
        "id": "RDczcy_hqL8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13 What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "-  Common techniques include:\n",
        "\n",
        "One-Hot Encoding: Creates binary (0/1) dummy variables for each category.\n",
        "\n",
        "Label Encoding: Assigns a unique integer to each category (use with caution if no inherent order).\n",
        "\n",
        "Ordinal Encoding: Assigns integers based on the order of categories (if an order exists).\n",
        "\n",
        "Effect (Deviation) Coding: Similar to dummy coding but compares each category to the overall mean."
      ],
      "metadata": {
        "id": "wb6Y3ktGqm_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14 What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "-  Interaction terms in Multiple Linear Regression allow the effect of one independent variable on the dependent variable to depend on the value of another independent variable."
      ],
      "metadata": {
        "id": "84eAZyzxqtHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15 How can the interpretation of the intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "-  In Simple Linear Regression: The intercept represents the expected value of the dependent variable when the single independent variable is zero.\n",
        "\n",
        "In Multiple Linear Regression: The intercept represents the expected value of the dependent variable when all independent variables in the model are zero."
      ],
      "metadata": {
        "id": "GwEAFTrLq7MH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16 What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "-  Significance of slope: The slope (coefficient 'm') indicates the magnitude and direction of the relationship between the independent and dependent variables. A positive slope means Y increases as X increases, a negative slope means Y decreases as X increases, and a slope near zero suggests a weak or no linear relationship."
      ],
      "metadata": {
        "id": "wxYLsW_IrKSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "The intercept provides the baseline value for the dependent variable when the independent variable(s) are zero. It sets the starting point of the regression line (or plane) and helps to understand the predicted value of the dependent variable when the predictors have no influence"
      ],
      "metadata": {
        "id": "N9PJcniurUrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "-  Limitations of using R² alone:\n",
        "\n",
        "Doesn't indicate causality: High R² doesn't mean X causes Y.\n",
        "\n",
        "Doesn't assess assumptions: R² doesn't tell you if model assumptions (linearity, etc.) are met.\n",
        "\n",
        "Doesn't indicate predictor importance: A high R² doesn't show which predictors are significant or how much each contributes."
      ],
      "metadata": {
        "id": "TM-5IZLXrcsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19 How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "-  A large standard error for a regression coefficient indicates high uncertainty about the estimated value of that coefficient.\n",
        "\n",
        "It means the estimated coefficient is less precise.\n",
        "\n",
        "The confidence interval for that coefficient will be wider.\n",
        "\n",
        "It is less likely that the coefficient is statistically significant (the p-value will tend to be larger)."
      ],
      "metadata": {
        "id": "D2JZl0q4rnzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20 How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "-  Identifying heteroscedasticity: In a residual plot (residuals vs. fitted values or residuals vs. an independent variable), heteroscedasticity is indicated by a non-random pattern in the spread of the residuals. Common patterns include:\n",
        "\n",
        "A fanning-out or fanning-in shape (cone shape).\n",
        "\n",
        "A bow-tie or hourglass shape.\n",
        "\n",
        "A systematic increase or decrease in the spread of residuals as the predicted value or independent variable increases."
      ],
      "metadata": {
        "id": "9LqB3K0VrxTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#21 What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "-  A high R² and a low adjusted R² usually indicate that you have added too many independent variables to the model, especially variables that are not significantly related to the dependent variable.\n",
        "\n",
        "  R² increases every time you add a predictor, even if it doesn't improve the model's fit significantly.\n",
        "\n",
        "  Adjusted R² penalizes the inclusion of unnecessary predictors. It only increases if the new predictor improves the model more than would be expected by chance."
      ],
      "metadata": {
        "id": "1JbloTcKr_vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#22 Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "-  Scaling variables (like standardization or normalization) in Multiple Linear Regression is important for several reasons, although it doesn't affect the estimated coefficients or the model's R² itself."
      ],
      "metadata": {
        "id": "H776WkaSsNy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#23 What is polynomial regression?\n",
        "\n",
        "-  Polynomial regression is a type of regression analysis where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an n-th degree polynomial."
      ],
      "metadata": {
        "id": "IqhmTayDsTyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#24 How does polynomial regression differ from linear regression?\n",
        "\n",
        "-  The main difference lies in the form of the relationship they model:\n",
        "\n",
        "Linear Regression models a straight-line relationship between the dependent and independent variables ($Y = mX + c$$Y = mX + c$).\n",
        "\n",
        "Polynomial Regression models a curved relationship by including polynomial terms (like $X^2$$X^2$, $X^3$$X^3$, etc.) in the equation."
      ],
      "metadata": {
        "id": "bObXFv2UstYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#25 When is polynomial regression used?\n",
        "\n",
        "-  Polynomial regression is used when the relationship between the independent and dependent variables appears to be curved rather than linear."
      ],
      "metadata": {
        "id": "kSWGSA-3s9QM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#26 What is the general equation for polynomial regression?\n",
        "\n",
        "-  The general equation for a polynomial regression with one independent variable X and a polynomial degree 'n' is:\n",
        "\n",
        "$Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + ... + \\beta_nX^n + \\epsilon$$Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + ... + \\beta_nX^n + \\epsilon$"
      ],
      "metadata": {
        "id": "ip1vji5NtPIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#27 Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "-  Yes, polynomial regression can be applied to multiple independent variables. This is done by including polynomial terms for each independent variable and potentially interaction terms between the independent variables raised to different powers"
      ],
      "metadata": {
        "id": "5ggEW2zjtVJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#28 What are the limitations of polynomial regression?\n",
        "\n",
        "-  Limitations of polynomial regression include:\n",
        "\n",
        "Overfitting: High-degree polynomials can fit the training data very closely but may not generalize well to new data.\n",
        "\n",
        "Extrapolation issues: Polynomial models can behave erratically when making predictions outside the range of the training data.\n",
        "\n",
        "Interpretation complexity: The coefficients of polynomial terms can be harder to interpret than those in linear regression"
      ],
      "metadata": {
        "id": "BVFvPynOtbRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#29 What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "-  Methods to evaluate model fit when selecting the degree of a polynomial include:\n",
        "\n",
        "Adjusted R²: A higher adjusted R² indicates a better fit while accounting for the number of predictors.\n",
        "\n",
        "Cross-Validation: Splitting data into training and testing sets to see how well the model generalizes to unseen data (e.g., k-fold cross-validation). Choose the degree that performs best on the testing/validation set.\n",
        "\n",
        "ANOVA (Analysis of Variance): Comparing models with different polynomial degrees to see if the higher-degree model significantly improves the fit."
      ],
      "metadata": {
        "id": "tCVFPTE_tmdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#30 Why is visualization important in polynomial regression?\n",
        "\n",
        "-  Visualization is important in polynomial regression for several reasons:\n",
        "\n",
        "Identifying Non-Linearity: Scatter plots of the data can visually reveal if a linear relationship is inadequate and if a curved (polynomial) relationship might be more appropriate.\n",
        "\n",
        "Choosing Polynomial Degree: Visualizing the fitted polynomial curves with different degrees can help you see which degree best captures the trend in the data without overfitting. You can compare how well a linear, quadratic, cubic, etc., curve fits the points."
      ],
      "metadata": {
        "id": "59kOlXOVtxJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#31  How is polynomial regression implemented in Python?\n",
        "-  Using PolynomialFeatures (from scikit-learn) to create polynomial terms ($X^2, X^3$$X^2, X^3$, etc.) from your independent variable(s).\n",
        "Using a standard LinearRegression model (from scikit-learn) to fit the dependent variable to these new polynomial features."
      ],
      "metadata": {
        "id": "Rn-k_JMvuK2m"
      }
    }
  ]
}